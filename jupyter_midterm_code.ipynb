{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a63a0e-203f-4458-b189-b1fb0b97c423",
   "metadata": {},
   "source": [
    "# Midterm Project - Saketh Lakshmanan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d52596-9737-40c4-aea6-f5a78db97ced",
   "metadata": {},
   "source": [
    "## First we write code for the BRUTE FORCE ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37c703-0f92-412a-bc45-9fdab1966783",
   "metadata": {},
   "source": [
    "### Load and collect all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9cbcbe-1ad2-47a4-8f65-e3ff7a2ad52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "from itertools import chain, combinations\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def load_csv(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        return list(reader)\n",
    "\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1849206-72c2-4f95-aba0-cbd752703bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a dataset to analyze: Amazon, Best Buy, Kmart, Nike, or General:  kmart\n",
      "Enter the minimum support count (integer, percent between 1 and 100):  40\n",
      "Enter the minimum confidence (decimal, percent between 0 and 100):  45\n"
     ]
    }
   ],
   "source": [
    "list_input = input(\"Enter a dataset to analyze: Amazon, Best Buy, Kmart, Nike, or General: \")\n",
    "if list_input.lower() == 'amazon':\n",
    "    itemset_list = pd.read_csv('amazonlist.csv')\n",
    "    the_transactions = pd.read_csv('amazontransactions.csv')\n",
    "elif list_input.lower() == 'best buy':\n",
    "    itemset_list = pd.read_csv('bestbuylist.csv')\n",
    "    the_transactions = pd.read_csv('bestbuytransactions.csv')\n",
    "elif list_input.lower() in ['kmart', 'k-mart']:\n",
    "    itemset_list = pd.read_csv('kmartlist.csv')\n",
    "    the_transactions = pd.read_csv('kmarttransactions.csv')\n",
    "elif list_input.lower() == 'nike':\n",
    "    itemset_list = pd.read_csv('nikelist.csv')\n",
    "    the_transactions = pd.read_csv('niketransactions.csv')\n",
    "elif list_input.lower() == 'general':\n",
    "    itemset_list = pd.read_csv('generallist.csv')\n",
    "    the_transactions = pd.read_csv('generaltransactions.csv')\n",
    "else:\n",
    "    print(\"Enter Valid Info (correct spelling)\")\n",
    "    sys.exit()\n",
    "\n",
    "order = sorted(itemset_list['ItemName'])\n",
    "\n",
    "dataset = []\n",
    "for lines in the_transactions['Transaction']:\n",
    "    trans = list(lines.strip().split(','))\n",
    "    trans1 = list(np.unique(trans))\n",
    "    trans1.sort(key=lambda x: order.index(x.strip()))\n",
    "    dataset.append(sorted(trans1))\n",
    "\n",
    "transaction_num = len(dataset)\n",
    "\n",
    "min_support = int(input(\"Enter the minimum support count (integer, percent between 1 and 100): \"))\n",
    "min_confidence = float(input(\"Enter the minimum confidence (decimal, percent between 0 and 100): \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fb485-7707-4a11-9b3e-87eea4000474",
   "metadata": {},
   "source": [
    "### Initialize the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd59b4b9-834f-46ce-8f00-4c6ebf62b2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Itemsets: {1: [['Bed Skirts'], ['Bedding Collections'], ['Bedspreads'], ['Decorative Pillows'], ['Embroidered Bedspread'], ['Kids Bedding'], ['Quilts'], ['Shams'], ['Sheets'], ['Towels']]}\n"
     ]
    }
   ],
   "source": [
    "candidate_itemsets = {}               \n",
    "freq_itemsets = {}    \n",
    "support_count_L = {}   \n",
    "itemset_size = 1     # set the initial itemset size\n",
    "non_frequent = {itemset_size: []}\n",
    "\n",
    "# populate candidate itemsets with singleton itemsets\n",
    "candidate_itemsets[itemset_size] = [[f] for f in order]\n",
    "\n",
    "# print the initialized candidate dictionary for verification\n",
    "print(\"Candidate Itemsets:\", candidate_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de5819-3b77-4043-a34c-e14c46dbad36",
   "metadata": {},
   "source": [
    "### Define a function for finding frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f960dbf6-8de9-4362-be46-3b30297e5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_itemsets(candidate_itemsets, transactions, minimum_support, non_frequent):\n",
    "    # Initialize lists to store frequent itemsets, their support counts, and new non-frequent itemsets\n",
    "    frequent_itemsets_list = []\n",
    "    support_count_list = []\n",
    "    new_non_frequent = []\n",
    "    \n",
    "    # Get the total number of transactions in the dataset\n",
    "    transaction_num = len(transactions)\n",
    "    # Get the current itemset size\n",
    "    K = len(non_frequent.keys())\n",
    "    \n",
    "    # Iterate through each candidate itemset\n",
    "    for i in range(0, len(candidate_itemsets)):\n",
    "        # Flag to check if the candidate itemset is a subset of any non-frequent itemset\n",
    "        is_candidate_set = 0\n",
    "        \n",
    "        # Check against non-frequent itemsets of the previous size (if any)\n",
    "        if K > 0:\n",
    "            for j in non_frequent[K]:\n",
    "                if set(j).issubset(set(candidate_itemsets[i])):\n",
    "                    is_candidate_set = 1\n",
    "                    break\n",
    "        \n",
    "        # If the candidate itemset is not a subset of any non-frequent itemset, proceed\n",
    "        if is_candidate_set == 0:\n",
    "            # Count the occurrences of the candidate itemset in the transactions\n",
    "            frequent_count = count_items(candidate_itemsets[i], transactions)\n",
    "            \n",
    "            # Check if the frequent count meets the minimum support threshold\n",
    "            if frequent_count >= (minimum_support / 100) * transaction_num:\n",
    "                # Append the frequent itemset and its support count to the lists\n",
    "                frequent_itemsets_list.append(candidate_itemsets[i])\n",
    "                support_count_list.append(frequent_count)\n",
    "            else:\n",
    "                # If not frequent, add the itemset to the list of new non-frequent itemsets\n",
    "                new_non_frequent.append(candidate_itemsets[i])\n",
    "    \n",
    "    # Return the lists containing frequent itemsets, their support counts, and new non-frequent itemsets\n",
    "    return frequent_itemsets_list, support_count_list, new_non_frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77814e-7b62-400d-8936-4ebd10a3dab3",
   "metadata": {},
   "source": [
    "### Define a function for counting items and a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f028f9-2dc4-47dd-90d1-9bcd8a74839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_items(candidate_itemset, transactions):\n",
    "    count = 0\n",
    "    for i in range (0, len(transactions)):\n",
    "        if set(candidate_itemset).issubset(set(transactions[i])):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098d9b7b-7e43-47ec-84a1-dbe8730a4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table (table, support_count):\n",
    "    print(\"Itemset | Count\")\n",
    "    for i in range (0, len(table)):\n",
    "        print(\"{} : {}\".format(table[i], support_count[i]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b6414-9ccf-4f3a-8062-b11319fc4975",
   "metadata": {},
   "source": [
    "### Define a function to link together itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7864c5b6-03bb-424d-aa55-3092015df0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_itemsets(itemset1, itemset2, order):\n",
    "    itemset1.sort(key=lambda x: order.index(x))\n",
    "    itemset2.sort(key=lambda x: order.index(x))\n",
    "    \n",
    "    for i in range(len(itemset1) - 1):\n",
    "        if itemset1[i] != itemset2[i]:\n",
    "            return []\n",
    "    \n",
    "    if order.index(itemset1[-1]) < order.index(itemset2[-1]):\n",
    "        return itemset1 + [itemset2[-1]]\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835394e-378b-41ba-a18a-5181327125ee",
   "metadata": {},
   "source": [
    "### Define a function to find the candidate itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d7fe7d-c4c0-42e9-a7fd-08f0323719d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_set(frequent_itemsets, order):\n",
    "    candidate_set = []\n",
    "    for i in range(len(frequent_itemsets)):\n",
    "        for j in range(i + 1, len(frequent_itemsets)):\n",
    "            joined_itemset = join_itemsets(frequent_itemsets[i], frequent_itemsets[j], order)\n",
    "            \n",
    "            if len(joined_itemset) > 0:\n",
    "                candidate_set.append(joined_itemset)\n",
    "    \n",
    "    return candidate_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a49bb-39f3-4719-b851-607092c1449a",
   "metadata": {},
   "source": [
    "### Define a function to find all possible subsets of a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a25b994-d206-40e5-adbc-ff65d87a63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "\n",
    "def possible_subsets(s):\n",
    "    a = list(s)\n",
    "    subsets = list(chain.from_iterable(combinations(a, r) for r in range(1, len(a) + 1)))\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5a033-a469-4714-b5fc-4c78b30e2886",
   "metadata": {},
   "source": [
    "### Rule formatting to look nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c0964a6-d298-4e4a-b6dc-632f4fa3fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_association_rules(lhs_itemset, rhs_itemset, confidence, support, transaction_num, rule_number):\n",
    "    association_rules_str = (\n",
    "        f\"Rule {rule_number}: {list(lhs_itemset)} -> {list(rhs_itemset)}\\n\"\n",
    "        f\"Confidence: {confidence * 100:.2f}%\\n\"\n",
    "        f\"Support: {(support / transaction_num) * 100:.2f}%\\n\\n\"\n",
    "    )\n",
    "    return association_rules_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9be3c-772e-456e-a975-c18f9ffcd74b",
   "metadata": {},
   "source": [
    "### Find the frequent itemsets and the corresponding support then update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7a13e3-77dd-4375-9f7f-6261bb38ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_set, support, new_non_frequent = frequent_itemsets(candidate_itemsets[itemset_size], dataset, min_support, non_frequent)\n",
    "\n",
    "freq_itemsets[itemset_size] = frequent_set\n",
    "non_frequent[itemset_size] = new_non_frequent\n",
    "support_count_L[itemset_size] = support           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66b1a8-12ac-4a2f-9641-a3e3416a6a68",
   "metadata": {},
   "source": [
    "### Define and print a table for itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4625c182-97e2-41ea-92a1-971658042aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_itemset_table(itemset_type, itemsets, support_counts):\n",
    "    print(f\"\\nTable {itemset_type}:\\n\")\n",
    "    for i, itemset in enumerate(itemsets):\n",
    "        itemset_str = ','.join(itemset)\n",
    "        count = support_counts[i]\n",
    "        print(f\"{itemset_str} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f5c7356-8999-43bf-b0cb-3c68e4673362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table Candidate:\n",
      "\n",
      "Bed Skirts : 11\n",
      "Bedding Collections : 7\n",
      "Bedspreads : 7\n",
      "Decorative Pillows : 10\n",
      "Embroidered Bedspread : 6\n",
      "Kids Bedding : 12\n",
      "Quilts : 8\n",
      "Shams : 10\n",
      "Sheets : 10\n",
      "Towels : 0\n",
      "\n",
      "Table Frequent:\n",
      "\n",
      "Bed Skirts : 11\n",
      "Decorative Pillows : 10\n",
      "Kids Bedding : 12\n",
      "Quilts : 8\n",
      "Shams : 10\n",
      "Sheets : 10\n"
     ]
    }
   ],
   "source": [
    "print_itemset_table(\"Candidate\", candidate_itemsets[1], [count_items(item, dataset) for item in candidate_itemsets[1]])\n",
    "print_itemset_table(\"Frequent\", freq_itemsets[1], support_count_L[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a59d6c-eda6-4c5c-a943-085c2a2d5006",
   "metadata": {},
   "source": [
    "### Generate candidate itemsets iteratively using the BRUTE FORCE  ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538f98a1-7802-4133-b9cf-1db3fb046eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Candidate Itemsets 2: \n",
      "\n",
      "Itemset | Count\n",
      "['Bed Skirts', 'Decorative Pillows'] : 4\n",
      "['Bed Skirts', 'Kids Bedding'] : 10\n",
      "['Bed Skirts', 'Quilts'] : 0\n",
      "['Bed Skirts', 'Shams'] : 8\n",
      "['Bed Skirts', 'Sheets'] : 9\n",
      "['Decorative Pillows', 'Kids Bedding'] : 4\n",
      "['Decorative Pillows', 'Quilts'] : 6\n",
      "['Decorative Pillows', 'Shams'] : 4\n",
      "['Decorative Pillows', 'Sheets'] : 2\n",
      "['Kids Bedding', 'Quilts'] : 2\n",
      "['Kids Bedding', 'Shams'] : 8\n",
      "['Kids Bedding', 'Sheets'] : 10\n",
      "['Quilts', 'Shams'] : 1\n",
      "['Quilts', 'Sheets'] : 1\n",
      "['Shams', 'Sheets'] : 6\n",
      "\n",
      "\n",
      "\n",
      "Table Frequent Itemsets 2:\n",
      "\n",
      "Itemset | Count\n",
      "['Bed Skirts', 'Kids Bedding'] : 10\n",
      "['Bed Skirts', 'Shams'] : 8\n",
      "['Bed Skirts', 'Sheets'] : 9\n",
      "['Kids Bedding', 'Shams'] : 8\n",
      "['Kids Bedding', 'Sheets'] : 10\n",
      "\n",
      "\n",
      "\n",
      "Table Candidate Itemsets 3: \n",
      "\n",
      "Itemset | Count\n",
      "['Bed Skirts', 'Kids Bedding', 'Shams'] : 7\n",
      "['Bed Skirts', 'Kids Bedding', 'Sheets'] : 9\n",
      "['Bed Skirts', 'Shams', 'Sheets'] : 6\n",
      "['Kids Bedding', 'Shams', 'Sheets'] : 6\n",
      "\n",
      "\n",
      "\n",
      "Table Frequent Itemsets 3:\n",
      "\n",
      "Itemset | Count\n",
      "['Bed Skirts', 'Kids Bedding', 'Sheets'] : 9\n",
      "\n",
      "\n",
      "\n",
      "Table Candidate Itemsets 4: \n",
      "\n",
      "Itemset | Count\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() ## Starting the time calc here\n",
    "# Initialize variables for the brute-force approach\n",
    "K = itemset_size + 1  # Increment the itemset size\n",
    "candidate_set = 0  # Flag to check if there are frequent itemsets\n",
    "candidate_itemsets = []  # List to store candidate itemsets at each iteration\n",
    "\n",
    "# Continue generating candidate itemsets until none are frequent for the current itemset size\n",
    "while candidate_set == 0:\n",
    "    # Generate candidate itemsets using the brute-force method\n",
    "    candid_itemsets = get_candidate_set(freq_itemsets[K-1], order)\n",
    "    # Append the generated candidate itemsets to the list\n",
    "    candidate_itemsets.append(candid_itemsets)\n",
    "    print(\"Table Candidate Itemsets {}: \\n\".format(K))\n",
    "    # Print the table of candidate itemsets along with their support counts\n",
    "    print_table(candid_itemsets, [count_items(item, dataset) for item in candid_itemsets])\n",
    "    \n",
    "    # Calculate frequent itemsets and update dictionaries\n",
    "    frequent_set, support, new_non_frequent = frequent_itemsets(candidate_itemsets[-1], dataset, min_support, non_frequent)\n",
    "    # Update dictionaries with the frequent itemsets, non-frequent itemsets, and support counts\n",
    "    freq_itemsets.update({K: frequent_set})\n",
    "    non_frequent.update({K: new_non_frequent})\n",
    "    support_count_L.update({K: support})\n",
    "    \n",
    "    # Check if there are no frequent itemsets for the current itemset size\n",
    "    if len(freq_itemsets[K]) == 0:\n",
    "        candidate_set = 1  # Set the flag to terminate the loop\n",
    "    else:\n",
    "        print(f\"Table Frequent Itemsets {K}:\\n\")\n",
    "        # Print the table of frequent itemsets along with their support counts\n",
    "        print_table(freq_itemsets[K], support_count_L[K])\n",
    "    \n",
    "    K += 1  # Increment the itemset size for the next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b4cc5-8bdc-4405-a072-8e5a3df2bc85",
   "metadata": {},
   "source": [
    "### Generate the final association rules for BRUTE FORCE ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b59ad5fe-4688-4598-88a4-125b1f7d0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Association Rules: \n",
      "\n",
      "Rule 1: ['Bed Skirts'] -> ['Kids Bedding']\n",
      "Confidence: 83.33%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 2: ['Kids Bedding'] -> ['Bed Skirts']\n",
      "Confidence: 90.91%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 3: ['Shams'] -> ['Bed Skirts']\n",
      "Confidence: 72.73%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 4: ['Bed Skirts'] -> ['Shams']\n",
      "Confidence: 80.00%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 5: ['Sheets'] -> ['Bed Skirts']\n",
      "Confidence: 81.82%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 6: ['Bed Skirts'] -> ['Sheets']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 7: ['Shams'] -> ['Kids Bedding']\n",
      "Confidence: 66.67%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 8: ['Kids Bedding'] -> ['Shams']\n",
      "Confidence: 80.00%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 9: ['Sheets'] -> ['Kids Bedding']\n",
      "Confidence: 83.33%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 10: ['Kids Bedding'] -> ['Sheets']\n",
      "Confidence: 100.00%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 11: ['Bed Skirts', 'Sheets'] -> ['Kids Bedding']\n",
      "Confidence: 75.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 12: ['Kids Bedding', 'Sheets'] -> ['Bed Skirts']\n",
      "Confidence: 81.82%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 13: ['Kids Bedding', 'Bed Skirts'] -> ['Sheets']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 14: ['Sheets'] -> ['Kids Bedding', 'Bed Skirts']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 15: ['Bed Skirts'] -> ['Kids Bedding', 'Sheets']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 16: ['Kids Bedding'] -> ['Bed Skirts', 'Sheets']\n",
      "Confidence: 100.00%\n",
      "Support: 45.00%\n",
      "\n",
      "\n",
      "\n",
      "Program execution time: 0.9253 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables for final association rules\n",
    "final_association_rules = \"\"\n",
    "rule_number = 1\n",
    "\n",
    "# Loop over each itemset size (starting from 1) in the frequent itemsets\n",
    "for itemset_size in range(1, len(freq_itemsets)):\n",
    "    # Loop over each itemset in the current itemset size\n",
    "    for itemset in freq_itemsets[itemset_size]:\n",
    "        # Generate all possible non-empty subsets of the current itemset\n",
    "        subsets = list(possible_subsets(set(itemset)))\n",
    "        subsets.pop()  # Remove the empty set\n",
    "\n",
    "        # Loop over each subset of the current itemset\n",
    "        for subset in subsets:\n",
    "            # Create item1 as a set representing the current subset\n",
    "            item1 = set(subset)\n",
    "            # Convert the current itemset to a set\n",
    "            itemset_set = set(itemset)\n",
    "            # Generate item2 as the complement of item1 in the itemset\n",
    "            item2 = itemset_set - item1\n",
    "            \n",
    "            # Calculate support counts for the itemsets and subsets\n",
    "            support_frequent_set = count_items(itemset_set, dataset)\n",
    "            support_item1 = count_items(item1, dataset)\n",
    "            support_item2 = count_items(item2, dataset)\n",
    "\n",
    "            # Calculate confidence of the association rule\n",
    "            confidence = support_frequent_set / support_item1\n",
    "\n",
    "            # Check if the confidence and support meet the user-defined thresholds\n",
    "            if confidence >= (min_confidence / 100) and support_frequent_set >= (min_support / 100) * transaction_num:\n",
    "                # Generate and append the association rule to the final result\n",
    "                final_association_rules += write_association_rules(item2, item1, confidence, support_frequent_set, transaction_num, rule_number)\n",
    "                # Increment the rule number for the next association rule\n",
    "                rule_number += 1\n",
    "\n",
    "end_time = time.time() ## Ending the time calc here\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the final association rules\n",
    "print(\"Final Association Rules: \\n\")\n",
    "print(final_association_rules)\n",
    "print(f\"\\nProgram execution time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa596b8-e0af-4a2c-a8e2-c1a1678fd778",
   "metadata": {},
   "source": [
    "## Verify the results with APRIORI built-in package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abc1b0bd-c797-4c63-a4af-f96d7facc637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Association Rules (Apriori): \n",
      "\n",
      "Rule 1: ['Bed Skirts'] -> ['Kids Bedding']\n",
      "Confidence: 83.33%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 2: ['Kids Bedding'] -> ['Bed Skirts']\n",
      "Confidence: 90.91%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 3: ['Shams'] -> ['Bed Skirts']\n",
      "Confidence: 72.73%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 4: ['Bed Skirts'] -> ['Shams']\n",
      "Confidence: 80.00%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 5: ['Sheets'] -> ['Bed Skirts']\n",
      "Confidence: 81.82%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 6: ['Bed Skirts'] -> ['Sheets']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 7: ['Shams'] -> ['Kids Bedding']\n",
      "Confidence: 66.67%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 8: ['Kids Bedding'] -> ['Shams']\n",
      "Confidence: 80.00%\n",
      "Support: 40.00%\n",
      "\n",
      "Rule 9: ['Sheets'] -> ['Kids Bedding']\n",
      "Confidence: 83.33%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 10: ['Kids Bedding'] -> ['Sheets']\n",
      "Confidence: 100.00%\n",
      "Support: 50.00%\n",
      "\n",
      "Rule 11: ['Sheets'] -> ['Kids Bedding', 'Bed Skirts']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 12: ['Bed Skirts'] -> ['Kids Bedding', 'Sheets']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 13: ['Kids Bedding'] -> ['Bed Skirts', 'Sheets']\n",
      "Confidence: 100.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 14: ['Bed Skirts', 'Sheets'] -> ['Kids Bedding']\n",
      "Confidence: 75.00%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 15: ['Kids Bedding', 'Sheets'] -> ['Bed Skirts']\n",
      "Confidence: 81.82%\n",
      "Support: 45.00%\n",
      "\n",
      "Rule 16: ['Kids Bedding', 'Bed Skirts'] -> ['Sheets']\n",
      "Confidence: 90.00%\n",
      "Support: 45.00%\n",
      "\n",
      "\n",
      "\n",
      "Program execution time: 0.0188 seconds\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori as mlxtend_apriori\n",
    "from mlxtend.frequent_patterns import association_rules as mlxtend_association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "start_time = time.time() ## Starting the time calc here\n",
    "\n",
    "# Use Apriori implementation from mlxtend\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "mlxtend_freq_itemsets = mlxtend_apriori(df, min_support=(min_support / 100), use_colnames=True)\n",
    "\n",
    "# Extract association rules from frequent itemsets using mlxtend\n",
    "mlxtend_rules = mlxtend_association_rules(mlxtend_freq_itemsets, metric=\"confidence\", min_threshold=min_confidence / 100)\n",
    "\n",
    "# Initialize variables for final association rules (mlxtend)\n",
    "final_association_rules_mlxtend = \"\"\n",
    "rule_number_mlxtend = 1\n",
    "\n",
    "# Loop over each rule generated by mlxtend\n",
    "for idx, row in mlxtend_rules.iterrows():\n",
    "    # Extract items and details from mlxtend result\n",
    "    item1_mlxtend = set(row['antecedents'])\n",
    "    item2_mlxtend = set(row['consequents'])\n",
    "    confidence_mlxtend = row['confidence']\n",
    "    support_mlxtend = row['support'] * transaction_num  # Convert support to count\n",
    "\n",
    "    # Check if the confidence and support meet the user-defined thresholds\n",
    "    if confidence_mlxtend >= (min_confidence / 100) and support_mlxtend >= (min_support / 100) * transaction_num:\n",
    "        # Generate and append the association rule to the final result (mlxtend)\n",
    "        final_association_rules_mlxtend += write_association_rules(item2_mlxtend, item1_mlxtend, confidence_mlxtend, support_mlxtend, transaction_num, rule_number_mlxtend)\n",
    "        # Increment the rule number for the next association rule\n",
    "        rule_number_mlxtend += 1\n",
    "\n",
    "end_time = time.time() ## Ending the time calc here\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the final association rules from mlxtend\n",
    "print(\"Final Association Rules (Apriori): \\n\")\n",
    "print(final_association_rules_mlxtend)\n",
    "print(f\"\\nProgram execution time: {elapsed_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba4edc-d943-40a2-87bc-38a231586995",
   "metadata": {},
   "source": [
    "## Verify Results using FP GROWTH built-in package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4cfcb52-ddc2-4a1a-8e5b-d3af2ce2acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequent Itemsets using FPGrowth:\n",
      "    support                            itemsets\n",
      "0      0.50                (Decorative Pillows)\n",
      "1      0.40                            (Quilts)\n",
      "2      0.60                      (Kids Bedding)\n",
      "3      0.55                        (Bed Skirts)\n",
      "4      0.50                            (Sheets)\n",
      "5      0.50                             (Shams)\n",
      "6      0.50          (Kids Bedding, Bed Skirts)\n",
      "7      0.50              (Kids Bedding, Sheets)\n",
      "8      0.45                (Bed Skirts, Sheets)\n",
      "9      0.45  (Kids Bedding, Bed Skirts, Sheets)\n",
      "10     0.40                 (Bed Skirts, Shams)\n",
      "11     0.40               (Kids Bedding, Shams)\n",
      "\n",
      "Association Rules using FPGrowth:\n",
      "                   antecedents                 consequents  \\\n",
      "0               (Kids Bedding)                (Bed Skirts)   \n",
      "1                 (Bed Skirts)              (Kids Bedding)   \n",
      "2               (Kids Bedding)                    (Sheets)   \n",
      "3                     (Sheets)              (Kids Bedding)   \n",
      "4                 (Bed Skirts)                    (Sheets)   \n",
      "5                     (Sheets)                (Bed Skirts)   \n",
      "6   (Kids Bedding, Bed Skirts)                    (Sheets)   \n",
      "7       (Kids Bedding, Sheets)                (Bed Skirts)   \n",
      "8         (Bed Skirts, Sheets)              (Kids Bedding)   \n",
      "9               (Kids Bedding)        (Bed Skirts, Sheets)   \n",
      "10                (Bed Skirts)      (Kids Bedding, Sheets)   \n",
      "11                    (Sheets)  (Kids Bedding, Bed Skirts)   \n",
      "12                (Bed Skirts)                     (Shams)   \n",
      "13                     (Shams)                (Bed Skirts)   \n",
      "14              (Kids Bedding)                     (Shams)   \n",
      "15                     (Shams)              (Kids Bedding)   \n",
      "\n",
      "    antecedent support  consequent support  support  confidence      lift  \\\n",
      "0                 0.60                0.55     0.50    0.833333  1.515152   \n",
      "1                 0.55                0.60     0.50    0.909091  1.515152   \n",
      "2                 0.60                0.50     0.50    0.833333  1.666667   \n",
      "3                 0.50                0.60     0.50    1.000000  1.666667   \n",
      "4                 0.55                0.50     0.45    0.818182  1.636364   \n",
      "5                 0.50                0.55     0.45    0.900000  1.636364   \n",
      "6                 0.50                0.50     0.45    0.900000  1.800000   \n",
      "7                 0.50                0.55     0.45    0.900000  1.636364   \n",
      "8                 0.45                0.60     0.45    1.000000  1.666667   \n",
      "9                 0.60                0.45     0.45    0.750000  1.666667   \n",
      "10                0.55                0.50     0.45    0.818182  1.636364   \n",
      "11                0.50                0.50     0.45    0.900000  1.800000   \n",
      "12                0.55                0.50     0.40    0.727273  1.454545   \n",
      "13                0.50                0.55     0.40    0.800000  1.454545   \n",
      "14                0.60                0.50     0.40    0.666667  1.333333   \n",
      "15                0.50                0.60     0.40    0.800000  1.333333   \n",
      "\n",
      "    leverage  conviction  zhangs_metric  \n",
      "0      0.170    2.700000       0.850000  \n",
      "1      0.170    4.400000       0.755556  \n",
      "2      0.200    3.000000       1.000000  \n",
      "3      0.200         inf       0.800000  \n",
      "4      0.175    2.750000       0.864198  \n",
      "5      0.175    4.500000       0.777778  \n",
      "6      0.200    5.000000       0.888889  \n",
      "7      0.175    4.500000       0.777778  \n",
      "8      0.180         inf       0.727273  \n",
      "9      0.180    2.200000       1.000000  \n",
      "10     0.175    2.750000       0.864198  \n",
      "11     0.200    5.000000       0.888889  \n",
      "12     0.125    1.833333       0.694444  \n",
      "13     0.125    2.250000       0.625000  \n",
      "14     0.100    1.500000       0.625000  \n",
      "15     0.100    2.000000       0.500000  \n",
      "\n",
      "Program execution time: 0.0126 seconds\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "start_time = time.time() ## Starting the time calc here\n",
    "\n",
    "# Use TransactionEncoder for one-hot encoding\n",
    "te = TransactionEncoder()\n",
    "one_hot_df = te.fit_transform(dataset)\n",
    "one_hot_df = pd.DataFrame(one_hot_df, columns=te.columns_)\n",
    "\n",
    "# Find frequent itemsets using FPGrowth\n",
    "frequent_itemsets_mlxtend = fpgrowth(one_hot_df, min_support=min_support / 100, use_colnames=True)\n",
    "print(f\"\\nFrequent Itemsets using FPGrowth:\")\n",
    "print(frequent_itemsets_mlxtend)\n",
    "\n",
    "# Generate association rules\n",
    "rules_mlxtend = association_rules(frequent_itemsets_mlxtend, metric=\"confidence\", min_threshold=min_confidence / 100)\n",
    "\n",
    "end_time = time.time() ## Ending the time calc here\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"\\nAssociation Rules using FPGrowth:\")\n",
    "print(rules_mlxtend)\n",
    "print(f\"\\nProgram execution time: {elapsed_time:.4f} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0196bf-662a-4d05-8135-45ef43e9f09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6 (main, Oct  2 2023, 20:46:14) [Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
